# Claude Code Open Configuration
# Generate this file with: cco config generate

# Server configuration
host: 127.0.0.1                # Host to bind to
port: 6970                     # Port to listen on
api_key: sk-api-00-XXXYYYZZZ   # Optional: API key to protect the proxy

# Provider configurations
providers:
  # OpenRouter - Access to multiple models from different providers
  - name: openrouter
    api_key: your-openrouter-api-key
    # url: https://openrouter.ai/api/v1/chat/completions  # Optional: URL is set automatically
    model_whitelist:       # Optional: restrict to specific model patterns
      - claude             # Allow any model containing "claude"
      - gpt-4             # Allow any model containing "gpt-4"

  # Local LM Studio instance
  - name: local-lmstudio
    url: "http://localhost:1234/v1/chat/completions"
    api_key: "not-needed"  # Local servers typically don't need auth

  # OpenAI - Direct access to GPT models
  - name: openai
    api_key: your-openai-api-key

  # Anthropic - Direct access to Claude models  
  - name: anthropic
    api_key: your-anthropic-api-key
    default_models:
      - claude-3-5-haiku-20241022
      - claude-3-5-sonnet-20240620
      - claude-3-5-sonnet-20241022
      - claude-3-7-sonnet-20250219
      - claude-3-haiku-20240307
      - claude-opus-4-20250514
      - claude-sonnet-4-20250514

  # Nvidia - Access to Nemotron models
  - name: nvidia
    api_key: your-nvidia-api-key

  # Google Gemini - Access to Gemini models
  - name: gemini
    api_key: your-gemini-api-key

  # Ollama - Local models (no API key needed)
  - name: ollama
    url: "http://localhost:11434/v1/chat/completions"
    api_key: "ollama"  # Ollama doesn't validate API keys
    default_models:
      - llama3.2
      - llama3.1
      - mistral
      - qwen2.5-coder

  # DeepSeek - Coding-focused models
  - name: deepseek
    api_key: your-deepseek-api-key
    default_models:
      - deepseek-coder
      - deepseek-reasoner

  # Groq - Ultra-fast inference
  - name: groq
    api_key: your-groq-api-key
    default_models:
      - llama-3.3-70b-versatile
      - mixtral-8x7b-32768

# Domain mappings - route local domains to existing providers
domain_mappings:
  localhost: openai       # Route localhost requests to OpenAI provider
  127.0.0.1: gemini      # Route 127.0.0.1 requests to Gemini provider  
  0.0.0.0: openrouter    # Route 0.0.0.0 requests to OpenRouter provider

# Router configuration for different use cases
router:
  default: local-lmstudio/qwen/qwen3-coder-30b           # Default to local model
  think: openai/o1-preview                               # For complex reasoning
  background: anthropic/claude-3-haiku-20240307         # For background tasks
  long_context: anthropic/claude-3-5-sonnet-20241022    # For long documents
  web_search: openrouter/perplexity/llama-3.1-sonar-huge-128k-online

# Features:
# - YAML takes precedence over JSON configuration
# - Default URLs are set automatically for all providers
# - Default models are populated for each provider
# - Model whitelist allows filtering available models
# - All 8 major LLM providers are supported (OpenRouter, OpenAI, Anthropic, Nvidia, Gemini, Ollama, DeepSeek, Groq)
# - Proxy can be protected with an API key
# - Different models can be configured for different use cases
# - domain_mappings allows routing local server requests to existing provider transformations
# - localhost requests will use OpenAI provider's request/response transformation
# - This enables local model support without needing a separate LocalProvider
# - You can map any domain to any configured or built-in provider
